{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1_CMAaJZUOnZXMU4hH0WquuVZq_hL7Vxo",
      "authorship_tag": "ABX9TyOr5NSrBCNDhLqZjmhoFwR4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rutavmehta/Skill/blob/main/Skill.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 1: Dataset Loading**\n",
        "\n",
        "Dataset loading is the process of importing data from a file (like CSV) into a structure (e.g., DataFrame in pandas) for analysis.\n",
        "\n",
        "Printing information involves displaying details about the dataset, such as its first few rows, data types, shape, and summary statistics to understand its structure and content."
      ],
      "metadata": {
        "id": "EhVpEqvrAriX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9fZmt3tkQUq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(r\"/content/drive/MyDrive/archive.csv\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\n===== FIRST FEW ROWS =====\")\n",
        "print(df.head(200))\n",
        "\n",
        "# Show dataset info and basic stats\n",
        "print(\"\\n===== DATAFRAME INFO =====\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\n===== DATA SUMMARY =====\")\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 2: Dataset Cleaning**\n",
        "\n",
        "Dataset cleaning is the process of fixing or removing incorrect, incomplete, duplicate, or irrelevant data to ensure the dataset is accurate, consistent, and ready for analysis. This includes handling missing values, duplicates, and formatting errors."
      ],
      "metadata": {
        "id": "_OQDMoa478uN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values and duplicates\n",
        "df.dropna(axis=0, inplace=True)\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Reset index for a clean look\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Check for any remaining null values or duplicates\n",
        "print(\"\\n===== NULL VALUE CHECK =====\")\n",
        "print(df.isnull().any())\n",
        "\n",
        "print(\"\\n===== NULL VALUE COUNT =====\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\n===== DUPLICATES CHECK =====\")\n",
        "print(df.duplicated().sum())\n"
      ],
      "metadata": {
        "id": "LETl8GFJ77Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 3 - Part 1: Label Encoding**\n",
        "\n",
        "Label encoding is the process of converting categorical data into numerical values, where each category is assigned a unique number — for example, \"Male\" becomes 0 and \"Female\" becomes 1. This helps machine learning models interpret categorical features that are ordinal or binary."
      ],
      "metadata": {
        "id": "qVPcljx5_rzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Define binary categorical columns\n",
        "binary_cols = ['Entrepreneurship']  # Assuming values are 'Yes' and 'No'\n",
        "\n",
        "# Apply Label Encoding ('Yes' -> 1, 'No' -> 0)\n",
        "label_encoder = LabelEncoder()\n",
        "for col in binary_cols:\n",
        "    df[col] = label_encoder.fit_transform(df[col])\n",
        "\n",
        "# Display dataset after label encoding\n",
        "print(\"\\n===== Label Encoded Dataset =====\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "2wV5gveR9_Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 3 - Part 2: OneHot Encoding**\n",
        "\n",
        "One-hot encoding is a technique that transforms categorical data into a series of binary columns, where each category gets its own column with values 0 or 1. For example, the \"Color\" feature with categories \"Red,\" \"Blue,\" and \"Green\" becomes three columns: Color_Red, Color_Blue, and Color_Green, where only one column is 1 (indicating the category) and the others are 0. This method prevents the model from misinterpreting categories as ordinal data."
      ],
      "metadata": {
        "id": "XEGJM01S8as1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define multi-class categorical columns\n",
        "multi_class_cols = ['Gender', 'Field_of_Study', 'Current_Job_Level']\n",
        "\n",
        "# Apply One-Hot Encoding (creates dummy variables)\n",
        "df_onehot = pd.get_dummies(df, columns=multi_class_cols, drop_first=True)\n",
        "\n",
        "# Display dataset after one-hot encoding\n",
        "print(\"\\n===== One-Hot Encoded Dataset =====\")\n",
        "print(df_onehot.head())\n"
      ],
      "metadata": {
        "id": "6GwvmXNu8gAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 4 - Part 1: Standard Scaling**\n",
        "\n",
        "Standard scaling is a data preprocessing technique that transforms numerical features to have a mean of 0 and a standard deviation of 1, ensuring all features contribute equally to the model."
      ],
      "metadata": {
        "id": "Pjl6h5Ze8vbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select numerical columns\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# Apply Standard Scaling\n",
        "scaler_standard = StandardScaler()\n",
        "df_standard_scaled = df.copy()\n",
        "df_standard_scaled[numerical_cols] = scaler_standard.fit_transform(df[numerical_cols])\n",
        "\n",
        "# Display the first few rows after Standard Scaling\n",
        "print(\"\\n===== Standard Scaled Data =====\")\n",
        "print(df_standard_scaled.head())\n"
      ],
      "metadata": {
        "id": "Yx3dDd768uhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 4 - Part 2: MinMax Scaling**\n",
        "\n",
        "Min-Max Scaling is a data preprocessing technique that transforms numerical features to a fixed range, usually 0 to 1, by subtracting the minimum value and dividing by the range (max - min), preserving the original data distribution."
      ],
      "metadata": {
        "id": "BLWLMM6P803t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Apply Min-Max Scaling\n",
        "scaler_minmax = MinMaxScaler()\n",
        "df_minmax_scaled = df.copy()\n",
        "df_minmax_scaled[numerical_cols] = scaler_minmax.fit_transform(df[numerical_cols])\n",
        "\n",
        "# Display the first few rows after Min-Max Scaling\n",
        "print(\"\\n===== Min-Max Scaled Data =====\")\n",
        "print(df_minmax_scaled.head())\n"
      ],
      "metadata": {
        "id": "1ql9Gu8T-fy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 4 - Part 3: Normalising**\n",
        "\n",
        "Normalization is a preprocessing technique that rescales data to have a standard range, typically between 0 and 1 or -1 and 1, ensuring all features contribute equally to the model and improving performance on algorithms that rely on distance measurements."
      ],
      "metadata": {
        "id": "iG_b02vLASgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# Identify numerical columns\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# Apply L2 Normalization\n",
        "normalizer = Normalizer(norm='l2')  # You can also use 'l1' or 'max' if needed\n",
        "df_normalized = df.copy()\n",
        "df_normalized[numerical_cols] = normalizer.fit_transform(df[numerical_cols])\n",
        "\n",
        "# Display the first few rows after normalization\n",
        "print(\"\\n===== Normalized Data =====\")\n",
        "print(df_normalized.head())\n"
      ],
      "metadata": {
        "id": "RPewVxhFAVrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 5 and 6: Plotting**\n",
        "\n",
        "Plotting is the process of visually representing data using charts, graphs, or plots to identify patterns, trends, and relationships, making data analysis easier and more intuitive.\n",
        "\n",
        "A histogram is a type of bar chart that shows the distribution of a dataset by grouping data into bins and counting the frequency of values in each bin.\n",
        "\n",
        "A scatter plot displays individual data points on a two-dimensional graph, showing the relationship between two variables to identify patterns, correlations, or outliers."
      ],
      "metadata": {
        "id": "0NxZnYP7GCx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualization: Plot histograms and scatterplots\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.histplot(df[\"Age\"], kde=True)\n",
        "plt.title(\"Age Distribution\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.histplot(df[\"SAT_Score\"])\n",
        "plt.title(\"SAT Score Distribution\")\n",
        "plt.show()\n",
        "\n",
        "# Scatter plot between SAT Score and University GPA colored by Career Satisfaction\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.scatterplot(data=df, x=\"SAT_Score\", y=\"University_GPA\", hue=\"Career_Satisfaction\")\n",
        "plt.title(\"SAT Score vs University GPA (Career Satisfaction)\")\n",
        "plt.show()\n",
        "\n",
        "# Scatter plot between High School GPA and University GPA colored by Career Satisfaction\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.scatterplot(data=df, x=\"High_School_GPA\", y=\"University_GPA\", hue=\"Career_Satisfaction\")\n",
        "plt.title(\"High School GPA vs University GPA (Career Satisfaction)\")\n",
        "plt.show()\n",
        "\n",
        "# Plot histogram for the entire dataframe\n",
        "plt.figure(figsize=(10, 10))\n",
        "df.hist(bins=\"auto\", figsize=(10, 10))\n",
        "plt.suptitle(\"Histograms of All Features\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "umm8wPQPmqjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Z-Score**"
      ],
      "metadata": {
        "id": "1mQCbzFI5Fbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "\n",
        "df = pd.read_csv(r\"/content/drive/MyDrive/archive.csv\")\n",
        "\n",
        "# Calculate Z-scores for each numeric column\n",
        "for col in df.columns:\n",
        "    if df[col].dtype in ['int64', 'float64']:\n",
        "        z_scores = zscore(df[col])\n",
        "        print(f\"Z-scores for column '{col}':\")\n",
        "        print(z_scores)\n",
        "\n",
        "# Function to remove outliers using IQR\n",
        "def remove_outliers_iqr(df):\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype in ['int64', 'float64']:  # Only process numerical columns\n",
        "            Q1 = df[col].quantile(0.25)\n",
        "            Q3 = df[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "            # Filter out outliers\n",
        "            df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
        "    return df\n",
        "\n",
        "# Apply the function to remove outliers\n",
        "df_no_outliers_iqr = remove_outliers_iqr(df)\n",
        "\n",
        "# Function to remove outliers using Z-score\n",
        "def remove_outliers_zscore(df, threshold=1):\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype in ['int64', 'float64']:  # Only process numerical columns\n",
        "            z_scores = zscore(df[col])\n",
        "            df = df[abs(z_scores) <= threshold]\n",
        "    return df\n",
        "\n",
        "# Apply the function to remove outliers\n",
        "df_no_outliers_zscore = remove_outliers_zscore(df)\n",
        "\n",
        "# Save cleaned data\n",
        "df_no_outliers_iqr.to_csv(r\"/content/drive/MyDrive/hi2_iqr.csv\", index=False)\n",
        "df_no_outliers_zscore.to_csv(r\"/content/drive/MyDrive/hi2_zscore.csv\", index=False)"
      ],
      "metadata": {
        "id": "ndJ_01nf-ZmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 7 and 8 - Filter Method: Pearsons Correlation Test**\n",
        "\n",
        "Pearson's correlation test measures the strength and direction of a linear relationship between two continuous variables, producing a value between -1 (perfect negative correlation) and +1 (perfect positive correlation), with 0 meaning no correlation."
      ],
      "metadata": {
        "id": "vq8i_7Mc3PDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Define numerical features and target column\n",
        "target_column = 'Starting_Salary'  # Replace with actual numeric target\n",
        "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# Compute Pearson correlation for each feature with target\n",
        "correlation_results = {}\n",
        "\n",
        "print(\"\\n===== Pearson Correlation Test =====\")\n",
        "for feature in numerical_features:\n",
        "    if feature != target_column:\n",
        "        corr, _ = pearsonr(df[feature], df[target_column])\n",
        "        correlation_results[feature] = corr\n",
        "        print(f\"Feature: {feature}, Correlation: {corr}\")\n",
        "\n",
        "# Apply threshold-based filtering\n",
        "threshold = 0.2  # Change this as needed (absolute correlation threshold)\n",
        "filtered_features = {k: v for k, v in correlation_results.items() if abs(v) >= threshold}\n",
        "\n",
        "# Display filtered features\n",
        "print(\"\\n===== Features Above Correlation Threshold =====\")\n",
        "for feature, corr in filtered_features.items():\n",
        "    print(f\"Feature: {feature}, Correlation: {corr}\")\n",
        "\n",
        "# Create correlation matrix\n",
        "correlation_matrix = df[numerical_features].corr()\n",
        "\n",
        "# Display correlation matrix\n",
        "print(\"\\n===== Correlation Matrix =====\")\n",
        "print(correlation_matrix)\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Feature Correlation Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wm6l3DhH3HPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 7 and 8 - Filter Method: ANOVA**\n",
        "\n",
        "ANOVA (Analysis of Variance) is a statistical test that compares the means of two or more groups to determine if there’s a significant difference between them, helping to find relationships between categorical independent variables and a continuous dependent variable."
      ],
      "metadata": {
        "id": "svDcFQ6V31S5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "# Define numerical features and target variable\n",
        "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "target_column = 'Job_Offers'\n",
        "\n",
        "# Ensure the target column is numeric (ANOVA requires numeric target categories)\n",
        "df[target_column] = pd.to_numeric(df[target_column])\n",
        "\n",
        "# Perform ANOVA F-Test\n",
        "f_scores, p_values = f_classif(df[numerical_features], df[target_column])\n",
        "\n",
        "# Display feature scores\n",
        "print(\"\\n===== ANOVA Feature Selection Results =====\")\n",
        "for feature, score, p_value in zip(numerical_features, f_scores, p_values):\n",
        "    print(f\"Feature: {feature}, Score: {score:.2f}, p-value: {p_value:.4f}\")\n",
        "\n",
        "# Optional: Filter features with a p-value less than 0.05 (statistically significant)\n",
        "significant_features = [feature for feature, p in zip(numerical_features, p_values) if p < 0.05]\n",
        "\n",
        "print(\"\\nStatistically Significant Features (p < 0.05):\")\n",
        "print(significant_features)\n"
      ],
      "metadata": {
        "id": "2wB-iwe633Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 7 and 8 - Filter Method: Chi-Square**\n",
        "\n",
        "The Chi-Square test is a statistical test that measures the association between categorical variables by comparing the observed and expected frequencies in a contingency table to determine if they are independent."
      ],
      "metadata": {
        "id": "yTkJQlJH4No8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Define categorical features and target variable\n",
        "categorical_features = ['Gender', 'Field_of_Study', 'Current_Job_Level', 'Entrepreneurship']\n",
        "target_column = 'Job_Offers'  # Ensure this is a categorical variable\n",
        "\n",
        "# Convert categorical features into numerical using one-hot encoding\n",
        "df_encoded = pd.get_dummies(df[categorical_features])\n",
        "\n",
        "# Perform Chi-Square test using SelectKBest\n",
        "k = 5  # Number of top features to select\n",
        "selector = SelectKBest(score_func=chi2, k='all')  # Initially select all\n",
        "X_new = selector.fit_transform(df_encoded, df[target_column])\n",
        "\n",
        "# Get feature scores\n",
        "feature_scores = selector.scores_\n",
        "threshold = np.percentile(feature_scores, 50)  # Select features above median score\n",
        "\n",
        "# Get top-K features\n",
        "top_k_selector = SelectKBest(score_func=chi2, k=k)\n",
        "X_k_new = top_k_selector.fit_transform(df_encoded, df[target_column])\n",
        "selected_k_features = np.array(df_encoded.columns)[top_k_selector.get_support()]\n",
        "top_k_scores = top_k_selector.scores_[top_k_selector.get_support()]\n",
        "\n",
        "# Display results for threshold-based selection\n",
        "print(\"\\n===== Chi-Square Features (Above Threshold) =====\")\n",
        "for feature, score in zip(df_encoded.columns, feature_scores):\n",
        "    if score >= threshold:\n",
        "        print(f\"Feature: {feature}, Score: {score}\")\n",
        "\n",
        "# Display results for top-K selection\n",
        "print(\"\\n===== Top-K Chi-Square Features =====\")\n",
        "for feature, score in zip(selected_k_features, top_k_scores):\n",
        "    print(f\"Feature: {feature}, Score: {score}\")\n"
      ],
      "metadata": {
        "id": "YUEKmroy4Cw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 7 and 8 - Filter Method: Information Gain**\n",
        "\n",
        "Information Gain measures how much a feature reduces uncertainty (entropy) about the target variable. It helps determine which features contribute the most to predicting the outcome by comparing the entropy before and after splitting the data based on that feature — the higher the gain, the more important the feature."
      ],
      "metadata": {
        "id": "7ASbdC5o4Z77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
        "\n",
        "# Define target variable\n",
        "target_column = 'Career_Satisfaction'  # Replace with actual categorical target\n",
        "\n",
        "# Encode categorical target\n",
        "df[target_column] = df[target_column].astype('category').cat.codes\n",
        "\n",
        "# Select numerical features\n",
        "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# Compute mutual information (information gain)\n",
        "mi_scores = mutual_info_classif(df[numerical_features], df[target_column])\n",
        "\n",
        "# Apply threshold-based filtering\n",
        "threshold = np.percentile(mi_scores, 50)  # Select features above the 50th percentile\n",
        "\n",
        "# Get top-K features\n",
        "k = 5  # Number of top features to select\n",
        "top_k_selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
        "X_k_new = top_k_selector.fit_transform(df[numerical_features], df[target_column])\n",
        "selected_k_features = np.array(numerical_features)[top_k_selector.get_support()]\n",
        "top_k_scores = top_k_selector.scores_[top_k_selector.get_support()]\n",
        "\n",
        "# Display results for threshold-based selection\n",
        "print(\"\\n===== Information Gain (Above Threshold) =====\")\n",
        "for feature, score in zip(numerical_features, mi_scores):\n",
        "    if score >= threshold:\n",
        "        print(f\"Feature: {feature}, Information Gain: {score}\")\n",
        "\n",
        "# Display results for top-K selection\n",
        "print(\"\\n===== Top-K Information Gain Features =====\")\n",
        "for feature, score in zip(selected_k_features, top_k_scores):\n",
        "    print(f\"Feature: {feature}, Information Gain: {score}\")\n"
      ],
      "metadata": {
        "id": "t-AuT-N44Dt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 9 and 10 - Wrapper Method: Forward Selection**\n",
        "\n",
        "Forward Selection is a step-by-step feature selection technique that starts with no features and adds one feature at a time — the one that improves the model performance the most — until adding more features no longer improves the model significantly."
      ],
      "metadata": {
        "id": "L8UFKvch14yU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('/content/drive/MyDrive/archive.csv')\n",
        "\n",
        "# Define features and target\n",
        "X = df.drop('Job_Offers', axis=1)\n",
        "y = df['Job_Offers']\n",
        "\n",
        "# Encode categorical columns\n",
        "for col in X.select_dtypes(include='object').columns:\n",
        "    X[col] = LabelEncoder().fit_transform(X[col])\n",
        "\n",
        "# Forward Selection\n",
        "selected_features = []\n",
        "remaining_features = list(X.columns)\n",
        "\n",
        "for _ in range(len(remaining_features)):\n",
        "    best_score = float('-inf')\n",
        "    best_feature = None\n",
        "\n",
        "    for feature in remaining_features:\n",
        "        trial_features = selected_features + [feature]\n",
        "        X_trial = sm.add_constant(X[trial_features])\n",
        "        model = sm.OLS(y, X_trial).fit()\n",
        "\n",
        "        if model.rsquared > best_score:\n",
        "            best_score = model.rsquared\n",
        "            best_feature = feature\n",
        "\n",
        "    if best_feature is not None:\n",
        "        selected_features.append(best_feature)\n",
        "        remaining_features.remove(best_feature)\n",
        "\n",
        "print(\"Selected Features (Forward Selection):\", selected_features)\n"
      ],
      "metadata": {
        "id": "jdYLpdoJ07is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 9 and 10 - Wrapper Method: Backward Elimination**\n",
        "\n",
        "Backward Elimination is a feature selection technique that starts with all features and removes the least significant one at each step — based on a statistical measure (like p-values in regression) — until only the most important features remain."
      ],
      "metadata": {
        "id": "UxECd105DFV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "# Start with all features\n",
        "features = list(X.columns)\n",
        "X_with_const = sm.add_constant(X[features])\n",
        "\n",
        "while len(features) > 0:\n",
        "    model = sm.OLS(y, X_with_const).fit()\n",
        "    p_values = model.pvalues[1:]\n",
        "\n",
        "    worst_p = p_values.idxmax()\n",
        "    if p_values[worst_p] > 0.05:\n",
        "        features.remove(worst_p)\n",
        "        X_with_const = sm.add_constant(X[features])\n",
        "    else:\n",
        "        break\n",
        "\n",
        "print(\"Selected Features (Backward Elimination):\", features)\n"
      ],
      "metadata": {
        "id": "5mcMxbMPDKF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 9 and 10 - Wrapper Method: Recursive Feature Elimination (RFE)**\n",
        "\n",
        "Recursive Feature Elimination (RFE) is a feature selection technique that recursively removes the least important features based on a model (like a classifier or regressor) until the desired number of features is reached, improving model performance."
      ],
      "metadata": {
        "id": "oMNmBprDDmJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor()\n",
        "rfe = RFE(model, n_features_to_select=5)\n",
        "fit = rfe.fit(X, y)\n",
        "\n",
        "selected_features = X.columns[fit.support_].tolist()\n",
        "print(\"Selected Features (RFE):\", selected_features)\n"
      ],
      "metadata": {
        "id": "taukE0o-DuE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 9 and 10 - Wrapper Method: Cross Validation (CV)**\n",
        "\n",
        "Cross-validation (CV) is a technique to evaluate a model’s performance by splitting the data into multiple parts — training on some parts and testing on the remaining — to ensure the model generalizes well to unseen data."
      ],
      "metadata": {
        "id": "N_cF7NTaDu1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor()\n",
        "cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
        "\n",
        "print(\"Cross-Validation Scores:\", cv_scores)\n",
        "print(\"Average CV Score:\", cv_scores.mean())\n"
      ],
      "metadata": {
        "id": "-35rr15nD1XB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}